---
title: "Circular Statistics with BUGS"
author: "João Neto"
date: September 2015
output: 
  html_document:
    toc: true
    toc_depth: 3
    fig_width: 8
    fig_height: 6
cache: yes
---

```{r, message=FALSE, warning=FALSE}
library(circular) # circular stats, cran.r-project.org/web/packages/circular/circular.pdf
```

Introduction to ploting tools
------------

Refs: 

+ [http://www.rpubs.com/mattbagg/circular](http://www.rpubs.com/mattbagg/circular)

Generate some random temporal data:

```{r}
library(lubridate)  # deal with date values

N <- 500
events <- as.POSIXct("2011-01-01", tz="GMT") + 
              days(floor(365*runif(N)))      + 
              hours(floor(24*rnorm(N)))      +  
              minutes(floor(60*runif(N))   ) +
              seconds(floor(60*runif(N)))
head(events)

df <- data.frame(datetime = events, eventhour = hour(events))
df$work <- df$eventhour %in% seq(9, 17) # determine if event is in business hours (9am to 17pm)
head(df,4)

# make our data into circular class from package circular
df$eventhour <- circular(df$eventhour%%24, # convert to 24 hrs
                         units="hours", 
                         template="clock24")
```

The package includes a rose diagram plot:

```{r}
par(mfrow=c(1,2))
rose.diag(df$eventhour, bins = 24, col = "lightblue", main = "Events by Hour (sqrt scale)",
          radii.scale = "sqrt", prop=3)

rose.diag(df$eventhour, bin = 24, col = "lightblue", main = "Events by Hour (linear scale)", 
    prop = 12, radii.scale = "linear", shrink = 1.25, cex = 0.8, ticks=TRUE)
par(mfrow=c(1,1))
```

We can use a typical plot to show a density:

```{r}
angles <- seq(0,2*pi,len=100)
data   <- dvonmises(circular(angles), mu=circular(pi/10), kappa=2)
plot(angles, data, type="l", col="blue")
```

And also draw a density in a circular plot:

```{r}
ff <- function(x) dvonmises(x, mu=circular(pi/10), kappa=20)
curve.circular(ff, col="blue", shrink=1.5, lwd=2)
```

Or draw an histogram using the rose diagrams together with the sample datapoints:

```{r}
set.seed(221)
rdata <- rvonmises(75, circular(3*pi/2), 2.5)
rose.diag(rdata, bins=50, prop=1.5, shrink=1)
points(rdata, pch=20, col="red")
```

<!--
And a circular density function:

```{r, eval=FALSE}
# bw.nrd0 implements a bandwidth of a Gaussian kernel density estimator
bw <- 10 * bw.nrd0(df$eventhour) # test other values
dens <- density.circular(df$eventhour, bw=bw)
plot(dens, plot.type = "l", join = TRUE, main = "Probability of Event by Hour",  xlab = "Hour", col="red")
```
-->

The von Mises Distribution
-----------------

The von Mises Distribution is a continuous probability distribution on the circle, it's the circular analogue of the normal distribution. The function has two parameters, a centrality one, $\mu$, and a concentration parameter, $\kappa$ (the higher, the more concentraded is the probability mass over $\mu$).

This is its pdf:

$$f(x|\mu,\kappa) = \frac{e^{\kappa \cos(x-\mu)}}{2\pi I_0(\kappa)}$$

where $$I_0(\kappa) = \frac{1}{\pi} \int_0^\pi e^{\kappa \cos(x)} dx$$

The Bessel function $I_0(\kappa)$ can be computed by R function `BesselI(kappa,0)` or approximated like this:

```{r, collapse=TRUE}
I0 <- function(kappa) besselI(kappa,0)

# Approximates Bessel(x,nu)
#  increase iter for better approximation
# cf. functions.wolfram.com/Bessel-TypeFunctions/BesselI/introductions/Bessels/05/
approx.I0 <- function(x, nu=0, iter=20) {
  result <- 1
  for(k in 1:iter)
    result = result + (x/2)^(2*k+nu) / (gamma(k+nu+1)*factorial(k))
  result
}

kappa <- 10
approx.I0(kappa,0)
I0(kappa)
I.0(kappa)  # package 'circular' also has this function
```

The next code creates a random sample at mean $\pi/4$:

```{r}
set.seed(101)
mu_real    <- pi/4
kappa_real <- 15
my_data    <- as.numeric(rvonmises(100, circular(mu_real), kappa_real))
```

Assuming the sample was generated by a von Mises distribution, we can easily compute the MLE estimation which is given by function `mle.vonmises`:

```{r, warning=FALSE, collapse=TRUE}
estimates <- mle.vonmises(as.circular(my_data))
estimates$mu[[1]]
estimates$kappa[[1]]
```

Let's plot the estimation vs the real signal:

```{r}
# plot two von Mises distributions
plot_circular_compare <- function(mu_real, kappa_real, mu_hat, kappa_hat, 
                                  shrink=1.75, col="red", add=FALSE) {
  ff.real     <- function(x) dvonmises(x, mu=circular(mu_real), kappa=kappa_real)
  ff.inferred <- function(x) dvonmises(x, mu=circular(mu_hat),  kappa=kappa_hat)
  curve.circular(ff.real,     col="blue", shrink=shrink, ylim=c(-0.75,1.25), lwd=2, add=add)
  curve.circular(ff.inferred, col=col,    lwd=2, add=T)
  legend("topleft",c("real","estimate"), col=c("blue",col), lwd=2, bty = "n")
}

plot_circular_compare(mu_real, kappa_real, estimates$mu[[1]], estimates$kappa[[1]])
```

The next sections show ways to solve this problem in a Bayesian setting using BUGS.

```{r, message=FALSE}
source("run_model.R") # import 'run.model()' to run bugs
```

BUGS model I: estimate mean direction with known concentration
---------------

The easier variant is when we know $\kappa$. The specification for this model in BUGS is:

```{r}
modelString = "
  model {

      for (i in 1:n) {

        ## phi[i] <- -log(exp(kappa * cos(x[i]-mu)) / (2*pi*I0)) + C   pdf simplifies to:
        phi[i] <- - kappa * cos(x[i]-mu) + log(2*pi*I0) + C
        
        dummy[i] <- 0
        dummy[i] ~ dpois( phi[i] )
      }

      # Priors 

      mu    ~ dunif(0,pi2)

      # other values

      C   <- 1000000   # for the zero's trick
      pi  <- 3.14159
      pi2 <- 2*pi
  }
"
```

Since the von Mises is not a distribution available in BUGS, we implemented its likelihood using the [zero's trick](http://users.aims.ac.za/~mackay/BUGS/Manuals/Tricks.html#SpecifyingANewSamplingDistribution).

With this model, we initialize parameter $\mu$ to its empirical mean (to help BUGS start with a good value), and run the model:

```{r}
# initializations
genInitFactory <- function()  {
  i <- 0
  function() {
    i <<- i + 1
    list( 
      mu = mean(my_data) # start with the mean estimate
    ) 
  }
}

# Everything is ready. Run the model!
run.model(modelString, samples=c("mu"), 
          data=list(I0    = I0(kappa_real),
                    x     = my_data,
                    kappa = kappa_real,
                    n     = length(my_data)), 
          chainLength=5e3, init.func=genInitFactory())

mu_hat <- samplesStats(c("mu"))[[1]]  # get point estimate of distribution's mean

# plot results
plot_circular_compare(mu_real, kappa_real, mu_hat, kappa_real)
```

BUGS model II: estimate mean direction using a MLE concentration
---------------

Assume we don't know $\kappa$ but approach the problem in a mixed way, by feeding the previous model with the empirical estimation of $\kappa$ based on the available sample. Another way to estimate the concentration parameter $\kappa$ is to use function `A1inv` to find the max-likelihood estimate:

```{r}
estimate_kappa <- function(data) {
  sum_sins <- sum(sin(data))
  sum_coss <- sum(cos(data))
  A1inv(mean(cos(data - atan2(sum_sins, sum_coss))))
}
```

So, being a simple extension of the previous model, would be to run it using this estimation of $\kappa$:

```{r}
kappa_hat <- estimate_kappa(my_data)

run.model(modelString, samples=c("mu"), 
          data=list(I0    = I0(kappa_hat),
                    x     = my_data,
                    kappa = kappa_hat,
                    n     = length(my_data)), 
          chainLength=5e3, init.func=genInitFactory())

mu_hat <- samplesStats(c("mu"))[[1]]  # get point estimate of distribution's mean

# plot results
plot_circular_compare(mu_real, kappa_real, mu_hat, kappa_hat)
```

BUGS model III: estimate mean direction and concentration
---------------

Here we will want to derive both parameters using BUGS.

The following solution is based on the fact that it is not easy to compute the Bessel function $I_0$ inside BUGS (which should be possible, but it is computationally expensive). So, we give BUGS a set of choices of $\kappa$ where the respective values $I_0(\kappa)$ were already computed in `R`.

The new model specification changed the log-likelihood. Now, instead of using a fixed $\kappa$ value, we use a variable `kappa_hat` which is computed via the random variable `k` following a categorical distribution over all possible pairs $\kappa, I_0(\kappa)$. These pairs initially start with equal probabilities.

```{r}
modelString = "
  model {

      for (i in 1:n) {

        phi[i] <- - kappa_hat * cos(x[i]-mu) + log(2*pi*IO_hat) + C
        
        dummy[i] <- 0
        dummy[i] ~ dpois( phi[i] )
      }

      k  ~ dcat(p[])

      for(j in 1:n_kappas) {
        p[j] <- 1/n_kappas         # creating vector p = {1/n_kappas,...}
      }

      kappa_hat <- kappas[k]
      IO_hat    <- I0s[k]

      # Priors 

      mu ~ dunif(0,pi2)

      # other values

      C   <- 1000000   # for the zero's trick
      pi  <- 3.14159
      pi2 <- 2*pi
  }
"
```

Let's give BUGS all values of $\kappa$ from 1 to 40:

```{r}
n_kappas <- 40
kappas <- 1:n_kappas
I0_s   <- sapply(kappas, I0)

# Everything is ready. Run the model!
run.model(modelString, samples=c("mu","kappa_hat"), 
          data=list(x        = my_data,
                    n        = length(my_data),
                    kappas   = kappas, 
                    I0s      = I0_s, 
                    n_kappas = n_kappas), 
          chainLength=5e3)

mu_hat    <- samplesStats(c("mu"))[[1]]  # point estimate of distribution's mean
kappa_hat <- samplesStats(c("kappa_hat"))[[1]]  # point estimate of distribution's concentration

# plot results
plot_circular_compare(mu_real, kappa_real, mu_hat, kappa_real)
```

Since we are making a Bayesian inference, we can also plot the uncertainty of these posterior values. First get the sample from the MCMC run:

```{r}
mus    <- samplesSample("mu")
kappas <- samplesSample("kappa_hat")
```

Then plot a bunch of them:

```{r}
ff.inferred <- function(x) dvonmises(x, mu=circular(mu_hat), kappa=kappa_hat)
curve.circular(ff.inferred, col="red", shrink=2.25, lwd=2) 
  
for (i in 1:250) {                                            # draw a number of samples
  ff.sample <- function(x) dvonmises(x, mu=circular(mus[i]), kappa=kappas[i])
  curve.circular(ff.sample, col="grey", lwd=2, add=T)
}

curve.circular(ff.inferred, col="red", lwd=2, add=T) # draw mean estimation
```

BUGS model IV: mixture of von Mises with known concentrations
---------------

The Bayesian framework can deal with mixtures in the same framework. Let's see a possible solution for this type of problem. We will present an example with two distributions, but it can be generalized for more complex mixtures. This solution assumes we know both concentration parameters $\kappa_1, \kappa_2$.

Let' create some random mix data:

```{r, warning=FALSE}
set.seed(101)

mu1_real    <- pi/4  # not known
kappa1_real <- 10    # known

mu2_real    <- pi    # not known
kappa2_real <- 5     # known

my_data1 <- as.numeric(rvonmises(100, circular(mu1_real), kappa1_real))
my_data2 <- as.numeric(rvonmises(100, circular(mu2_real), kappa2_real))

my_mixdata <- c(my_data1, my_data2)

par(mfrow=c(1,2))
rose.diag(my_mixdata, bins=50, prop=2.5, col="lightgreen")
points(as.circular(my_data1), pch=20, col="darkgreen")
points(as.circular(my_data2), pch=20, col="lightgreen")

ff.mix1 <- function(x) dvonmises(x, mu=circular(mu1_real), kappa=kappa1_real)
ff.mix2 <- function(x) dvonmises(x, mu=circular(mu2_real), kappa=kappa2_real)
curve.circular(ff.mix1, xlim=c(-1.5,1.5), col="darkgreen", shrink=1.1, lwd=2)
curve.circular(ff.mix2, col="lightgreen", lwd=2, add=T)
par(mfrow=c(1,1))
```

This model is based on the BUGS code for [mixture of gaussians](http://www.di.fc.ul.pt/~jpn/r/bugs/part2.html#mixture-of-gaussians):

```{r}
modelString = "
  model {

      for (i in 1:n) {

        phi[i] <- - kappa[i] * cos(x[i]-mu[i]) + log(2*pi*I0[i]) + C

        mu[i]    <- lambdaMu[G[i]]
        kappa[i] <- lambdaKappa[G[i]]
        I0[i]    <- lambdaI0[G[i]]
        
        G[i]   ~  dcat(P[])     # the cluster attributions for each x[i]

        dummy[i] <- 0
        dummy[i] ~ dpois( phi[i] )
      }

      P[1:2] ~ ddirch(alpha[])  # dirichlet distribution (here, just 2 clusters)
      alpha[1] <- 0.5           
      alpha[2] <- 0.5              

      lambdaKappa[1] <- 10
      lambdaKappa[2] <- 5

      lambdaI0[1] <- 2815.717   # I_0(10)
      lambdaI0[2] <- 27.23987   # I_0(5)

      lambdaMu[1] ~ dunif(0,pi2)
      lambdaMu[2] ~ dunif(0,pi2)

      # other values

      C   <- 1000000   # for the zero's trick
      pi  <- 3.14159
      pi2 <- 2*pi
  }
"
```

The vector `G` estimates to which cluster a data point belongs to. If a datapoint $x$ is assigned to cluster $i$, then its log-likelihood is given by $f(x|\mu_i, \kappa_i)$. Vector $G$ is modeled by a dirichlet distribution, where its parameter, $\alpha$, must be a simplex (ie, a vector of non negatives values that sum to one).

Let's run the model and extract the posterior means:

```{r}
run.model(modelString, samples=c("lambdaMu","G"), 
          data=list(x = my_mixdata,
                    n = length(my_mixdata),
                    G = c(1, rep(NA,length(my_mixdata)-1))), # need to give one value (?)
          chainLength=1e4)

mu_clusters <- samplesStats("lambdaMu")$mean
mu_clusters <- mu_clusters %% (2*pi)  # place values inside the [0,2*pi] interval
```

The next code compares the true mix with the estimated mix:

```{r}
ff.real1 <- function(x) dvonmises(x, mu=circular(mu1_real),  kappa=kappa1_real)
ff.real2 <- function(x) dvonmises(x, mu=circular(mu2_real),  kappa=kappa2_real)
ff.real  <- function(x) ifelse(ff.real1(x)>ff.real2(x),ff.real1(x),ff.real2(x))
curve.circular(ff.real, col="blue", shrink=2.25, lwd=2) 

ff.inferred1 <- function(x) dvonmises(x, mu=circular(mu_clusters[1]),  kappa=kappa1_real)
curve.circular(ff.inferred1, col="red", lwd=2, add=T) 
ff.inferred2 <- function(x) dvonmises(x, mu=circular(mu_clusters[2]),  kappa=kappa2_real)
ff.inferred  <- function(x) ifelse(ff.inferred1(x)>ff.inferred2(x),ff.inferred1(x),ff.inferred2(x))
curve.circular(ff.inferred, col="red", lwd=2, add=T) 

legend("topleft",c("real","estimate"), col=c("blue","red"), lwd=2, bty = "n")
```

The MCMC also gives an attribution of each datapoint to each cluster. We can check how precise was this clustering. In the next code snippet, we see that each point has a value that determines how much nearer it is from cluster $1$ or $2$. This value can be interpreted (in a logistic regression fashion) as a probability of belong to either cluster. Herein, we will just apply a thresold at the middle value, $1.5$, to classify each datapoint.

We denote the two real distributions as 'a' and 'b', and compare them with the estimated cluster id of each sample:

```{r, collapse=TRUE}
cluster_id <- samplesStats("G")[,1]
cluster_id
estimated_id <- 0+(cluster_id > 1.5) # values are between 1 and 2, we split at the middle

real_id <- c(rep('a',100), rep('b',100))[-1] # remove the first value (was fixed in BUGS)
table(estimated_id, real_id)
```

The classification error(s) were due to 'outliers' of the second distribution, which had a small $\kappa$ parameter, and so its values are more spread over the unit circle.

Let's visualize where the error(s) are (the color square gives its real cluster id, and the color cross the estimated id):

```{r, warning=FALSE}
mixdata_plot <- my_mixdata[-1]
rose.diag(my_mixdata, bins=50, prop=2.5, col="lightgreen",
          control.circle=circle.control(lty=0))
points(as.circular(mixdata_plot[real_id=='a']), col="orange", pch=0, lwd=1.5)
points(as.circular(mixdata_plot[real_id=='b']), col="purple", pch=0, lwd=1.5)

points(as.circular(mixdata_plot[estimated_id==0]), col="orange", pch=3, lwd=1.5)
points(as.circular(mixdata_plot[estimated_id==1]), col="purple", pch=3, lwd=1.5)
```

We see that the error is at one of the 'frontiers' between the influence of both von Mises distributions.

The Wrapped Cauchy Distribution
-----------------

The Wrapped Cauchy Distribution is the circular analogue of the Cauchy function, which s quite useful for heavy tail modeling. It has two parameters, a centrality, $\mu$, and a concentration parameter, $\rho$  (the higher, the more concentraded is the probability mass over $\mu$).

Its pdf is given by

$$f(\theta | \mu, \rho) = \frac{1}{2 \pi} \frac{1-\rho^2}{1+\rho^2-2\cos(\theta-\mu)}, -\pi \le \theta \lt \pi$$

where $\mu \in [-\pi,\pi)$ and $\rho \in [0,1)$

The next code samples 100 data points from $f(\cdot | 0, 0.7)$

```{r}
mu  <- 0
rho <- 0.7

set.seed(101)
rdata <- rwrappedcauchy(100, mu=circular(mu), rho=rho)
plot(rdata, xlim=c(-1,2), col="darkgreen", pch=20)
ff <- function(x) dwrappedcauchy(x, mu=circular(mu), rho=rho)
curve.circular(ff, col="blue", lwd=2, add=T)
```

Notice how the number of 'outliers' increase in this distribution.

The function `pdf_cauchy` computes the density just like circular package's `dwrappedcauchy`:

```{r, collapse=TRUE, warning=FALSE}
# pre: theta in -pi <= mu < pi
# location mu, -pi <= mu < pi
# concentration rho, rho in [0,1)
pdf_cauchy <- function(theta, mu, rho) {
  1/(2*pi) * (1-rho^2) / (1+rho^2-2*rho*cos(theta-mu))
}

mu  <- as.circular(pi/4)
rho <- 0.3
xs  <- as.circular(seq(0,pi/2,len=7))

dwrappedcauchy(xs, mu, rho) 
sapply(xs,function (x) pdf_cauchy(x,mu,rho))
```

Now let's see how to model this function in BUGS.

BUGS model I : estimate mean and concentration
----------

This pdf is easier to represent in BUGS, we can directly estimate both parameters:

```{r}
modelString = "
  model {

      for (i in 1:n) {

        phi[i] <- -log( (1-pow(rho,2)) / (2*pi*(1+pow(rho,2)-2*rho*cos(x[i]-mu))) ) + C

        dummy[i] <- 0
        dummy[i] ~ dpois( phi[i] )
      }

      # Priors 

      mu  ~ dunif(pi_1,pi)
      rho ~ dunif(0,1)

      # other values

      C    <- 1000000   # for the zero's trick
      pi   <- 3.14159
      pi_1 <- -pi
  }
"
```

Notice the weak priors over $\mu$ and $\rho$. As usual, if there is some previous information that would help us concentrate the prior mass, the results could be improved.

Now, let's create a random dataset:

```{r, warning=FALSE}
mu  <- as.circular(pi/4)
rho <- 0.7

set.seed(121)
n <- 100
my_data <- rwrappedcauchy(n, mu, rho)

rose.diag(my_data, bins=50, prop=1.5, shrink=1)
points(my_data, pch=20, col="darkgreen")
```

And apply the model to the dataset. Herein, we'll use the MLE estimates to initialize the values of the parameters to help BUGS (since they are easy to compute with function `mle.wrappedcauchy`, but we could initialize to any reasonable value). We will also use these MLE values to compare with the MCMC results.

```{r}
# pre: x>=0
# return circular value between -pi <= mu < pi
cauchy_process_val <- function(x) {
  while(x>pi)
    x = x-2*pi
  x
}

# the pdf requires values between -pi <= mu < pi
pp_my_data <- sapply(as.numeric(my_data), cauchy_process_val)

# MLE estimates
mle <- mle.wrappedcauchy(my_data)
mu_mle  <- as.numeric(mle$mu)
rho_mle <- as.numeric(mle$rho)

# initializations
genInitFactory <- function()  {
  i <- 0
  function() {
    i <<- i + 1
    list( 
      mu  = mu_mle,  # MLE estimates
      rho = rho_mle
    ) 
  }
}

# Everything is ready. Run the model!
run.model(modelString, samples=c("mu", "rho"), 
          data=list(x     = pp_my_data,
                    n     = length(pp_my_data)), 
          chainLength=2e4, init.func=genInitFactory())

samplesStats(c("mu"))
samplesStats(c("rho"))

# keep point estimates
mu_hat  <- samplesStats(c("mu"))[[1]]  # get point estimate of distribution's mean
rho_hat <- samplesStats(c("rho"))[[1]]  # get point estimate of distribution's mean
```

We can now plot the results:

```{r}
# plot results
plot(my_data, shrink=1.5, col="darkgreen", pch=20)

ff.real <- function(x) dwrappedcauchy(x, mu=circular(mu), rho=rho)
curve.circular(ff.real, col="blue", lwd=2, add=T)

ff.mle <- function(x) dwrappedcauchy(x, mu=circular(mu_mle), rho=rho_mle)
curve.circular(ff.mle, col="orange", lwd=2, add=T)

ff.inferred <- function(x) dwrappedcauchy(x, mu=circular(mu_hat), rho=rho_hat)
curve.circular(ff.inferred, col="red", lwd=2, add=T)
legend("topleft",c("real", "MLE", "MCMC"), col=c("blue","orange", "red"), lwd=2, bty = "n")
```

Again, we could also check the uncertainty over the mean estimate:

```{r}
mus  <- samplesSample("mu")
rhos <- samplesSample("rho")

ff.inferred <- function(x) dwrappedcauchy(x, mu=circular(mu_hat), rho=rho_hat)
curve.circular(ff.inferred, col="red", shrink=1.5, lwd=2) 
  
for (i in 1:500) {                                         # draw a number of samples
  ff.sample <- function(x) dwrappedcauchy(x, mu=circular(mus[i]), rho=rhos[i])
  curve.circular(ff.sample, col="grey", lwd=2, add=T)
}

curve.circular(ff.inferred, col="red", lwd=2, add=T) # draw mean estimation
```

```{r, eval=FALSE, echo=FALSE}
# Model for known rho

modelString = "
  model {

      for (i in 1:n) {

        phi[i] <- -log( (1-pow(rho,2)) / (2*pi*(1+pow(rho,2)-2*rho*cos(x[i]-mu))) ) + C

        dummy[i] <- 0
        dummy[i] ~ dpois( phi[i] )
      }

      # Priors 

      mu  ~ dunif(pi_1,pi)

      # other values

      C    <- 1000000   # for the zero's trick
      pi   <- 3.14159
      pi_1 <- -pi
  }
"

# initializations
genInitFactory <- function()  {
  i <- 0
  function() {
    i <<- i + 1
    list( 
      mu = mu_mle
    ) 
  }
}

# Everything is ready. Run the model!
run.model(modelString, samples=c("mu", "rho"), 
          data=list(x     = pp_my_data,
                    rho   = rho,
                    n     = length(pp_my_data)), 
          chainLength=1e4, init.func=genInitFactory())

samplesStats(c("mu"))

mu_hat  <- samplesStats(c("mu"))[[1]]  # get point estimate of distribution's mean
rho_hat <- samplesStats(c("rho"))[[1]]  # get point estimate of distribution's mean

# plot results
plot(my_data, xlim=c(-1,2.9), col="green", pch=20)

ff.real <- function(x) dwrappedcauchy(x, mu=circular(mu), rho=rho)
curve.circular(ff.real, col="blue", lwd=2, add=T)

ff.inferred <- function(x) dwrappedcauchy(x, mu=circular(mu_hat), rho=rho)
curve.circular(ff.inferred, col="red", lwd=2, add=T)
legend("topleft",c("real", "MLE", "MCMC"), col=c("blue","grey", "red"), lwd=2, bty = "n")
```

BUGS model II: mixture of wrapped Cauchys
----------

Make a mixture:

```{r, warning=FALSE}
set.seed(101)

mu1_real  <- pi/4   # not known
rho1_real <- 0.65   # not known

mu2_real  <- -pi/2  # not known
rho2_real <- 0.75   # not known

my_data1 <- as.numeric(rwrappedcauchy(100, circular(mu1_real), rho1_real))
my_data2 <- as.numeric(rwrappedcauchy(100, circular(mu2_real), rho2_real))

my_mixdata <- c(my_data1, my_data2)

par(mfrow=c(1,2))
rose.diag(my_mixdata, bins=50, prop=2.5, col="lightgreen")
points(as.circular(my_data1), pch=20, col="darkgreen")
points(as.circular(my_data2), pch=20, col="lightgreen")

ff.mix1 <- function(x) dwrappedcauchy(x, mu=circular(mu1_real), rho=rho1_real)
ff.mix2 <- function(x) dwrappedcauchy(x, mu=circular(mu2_real), rho=rho2_real)
curve.circular(ff.mix1, xlim=c(-1.9,1.1), col="darkgreen", shrink=1.1, lwd=2)
curve.circular(ff.mix2, col="lightgreen", lwd=2, add=T)
```

The model is very similar to the previous mixture for two von Mises distributions. We just added the concentration parameter $\rho$ as another parameter for the MCMC to estimate:

```{r}
modelString = "
  model {

      # Log-Likelihood function 

      for (i in 1:n) {

        phi[i] <- -log( (1-pow(rho[i],2)) /
                        (2*pi*(1+pow(rho[i],2)-2*rho[i]*cos(x[i]-mu[i]))) ) + C

        mu[i]  <- lambdaMu[G[i]]
        rho[i] <- lambdaRho[G[i]]

        G[i]   ~  dcat(P[])     # the cluster attributions for each x[i]

        dummy[i] <- 0
        dummy[i] ~ dpois( phi[i] )
      }

      P[1:2] ~ ddirch(alpha[])  # dirichlet distribution (here, just 2 clusters)
      alpha[1] <- 0.5           
      alpha[2] <- 0.5              

      lambdaMu[1] ~ dunif(pi_1,pi)
      lambdaMu[2] ~ dunif(pi_1,pi)

      lambdaRho[1] ~ dunif(0,1)
      lambdaRho[2] ~ dunif(0,1)

      # other values

      C   <- 1000000   # for the zero's trick
      pi  <- 3.14159
      pi_1 <- -pi
  }
"
```

Now let's run the model:

```{r}
pp_my_mixdata <- sapply(as.numeric(my_mixdata), cauchy_process_val)

run.model(modelString, samples=c("lambdaMu","lambdaRho","G"), 
          data=list(x = my_mixdata,
                    n = length(my_mixdata),
                    G = c(1, rep(NA,length(my_mixdata)-1))),
          chainLength=1e4)

mu_clusters  <- samplesStats("lambdaMu")$mean
rho_clusters <- samplesStats("lambdaRho")$mean
```

Let's compare the true and estimated mixtures:

```{r}
ff.real1 <- function(x) dwrappedcauchy(x, mu=circular(mu1_real),  rho=rho1_real)
ff.real2 <- function(x) dwrappedcauchy(x, mu=circular(mu2_real),  rho=rho2_real)
ff.real  <- function(x) ifelse(ff.real1(x)>ff.real2(x),ff.real1(x),ff.real2(x))
curve.circular(ff.real, col="blue", shrink=2.25, lwd=2) 

ff.inferred1 <- function(x) dwrappedcauchy(x, mu=circular(mu_clusters[1]),  rho=rho_clusters[1])
ff.inferred2 <- function(x) dwrappedcauchy(x, mu=circular(mu_clusters[2]),  rho=rho_clusters[2])
ff.inferred  <- function(x) ifelse(ff.inferred1(x)>ff.inferred2(x),ff.inferred1(x),ff.inferred2(x))
curve.circular(ff.inferred, col="red", lwd=2, add=T) 

legend("topleft",c("real","estimate"), col=c("blue","red"), lwd=2, bty = "n")
```

And, as we did with the von Mises case, let's check the cluster attribution. Before that, just notice that the classification error must be higher, since there are much more datapoints from one distribution in the density peak area of the other.

```{r, collapse=TRUE}
cluster_id <- samplesStats("G")[,1]
cluster_id
estimated_id <- 0+(cluster_id > 1.5) # values are between 1 and 2, we split at the middle

real_id <- c(rep('a',100), rep('b',100))[-1] # remove the first value (was fixed in BUGS)
table(estimated_id, real_id)
```

Let's visualize where the errors are (again, the color square gives its real cluster id, and the color cross the estimated id):

```{r, warning=FALSE}
mixdata_plot <- my_mixdata[-1]
rose.diag(my_mixdata, bins=50, prop=2.5, col="lightgreen",
          control.circle=circle.control(lty=0))
points(as.circular(mixdata_plot[real_id=='a']), col="orange", pch=0, lwd=1.5)
points(as.circular(mixdata_plot[real_id=='b']), col="purple", pch=0, lwd=1.5)

points(as.circular(mixdata_plot[estimated_id==0]), col="orange", pch=3, lwd=1.5)
points(as.circular(mixdata_plot[estimated_id==1]), col="purple", pch=3, lwd=1.5)
```

BUGS model III: mixture of von Mises and wrapped Cauchy
----------

In this section we will make a model to infer the parameters of a mixed signal from two different sources, one a von Mises distribution, another a wrapped Cauchy.

```{r, warning=FALSE}
set.seed(121)

mu1_real  <- pi/8   # not known
kappa1_real <- 25   # known

mu2_real  <- -pi/8  # not known
rho2_real <- 0.9    # not known

my_data1 <- as.numeric(rvonmises(100,      circular(mu1_real), kappa1_real))
my_data2 <- as.numeric(rwrappedcauchy(100, circular(mu2_real), rho2_real))

my_mixdata <- c(my_data1, my_data2)

par(mfrow=c(1,2))
rose.diag(my_mixdata, bins=50, prop=2.5, col="lightgreen")
points(as.circular(my_data1), pch=20, col="darkgreen")
points(as.circular(my_data2), pch=20, col="lightgreen")

ff.mix1 <- function(x) dvonmises(x,      mu=circular(mu1_real), kappa=kappa1_real)
ff.mix2 <- function(x) dwrappedcauchy(x, mu=circular(mu2_real), rho=rho2_real)
curve.circular(ff.mix1, xlim=c(-0.9,2.1), col="darkgreen", shrink=1.5, lwd=2)
curve.circular(ff.mix2, col="lightgreen", lwd=2, add=T)
par(mfrow=c(1,1))
```

We assume `G[i]==1` if datapoint $x_i$ was produced by the von Mises, and `G[i]==2` if produce by the wrapped Cauchy. The mixed likelihood is given as follows:

$$f(x_i|\mu_{VM},\kappa,\mu_{WC},\rho) = (G[i]==1)*f_{VM}(x_i|\mu_{VM},\kappa) + 
                                         (G[i]==2)*f_{WC}(x_i|\mu_{WC},\rho)$$
                                       
where $f_{VM}$ and $f_{WC}$ are the respective pdf's for the von Mises and wrapped Cauchy distributions.

We use BUGS' `equals` function to implement this mixed likelihood. The value of `equals(x,y)` is $1$ when `x` and `y` are equal, or $0$ otherwise.

```{r}
modelString = "
  model {

      for (i in 1:n) {

        phi[i] <- equals(G[i],1) *                  # von Mises pdf
                    (
                      - kappa[i] * cos(x[i]-mu[i]) + log(2*pi*I0[i])
                    )
                  +
                  equals(G[i],2) *                  # wrapped Cauchy
                    ( -log( (1-pow(rho[i],2)) /
                            (2*pi*(1+pow(rho[i],2)-2*rho[i]*cos(x[i]-mu[i]))) )
                    )
                  + C

        mu[i]    <- lambdaMu[G[i]]
        kappa[i] <- lambdaKappa[G[i]]
        I0[i]    <- lambdaI0[G[i]]
        rho[i]   <- lambdaRho[G[i]]
        
        G[i] ~ dcat(P[])           # the cluster attributions for each x[i]

        dummy[i] <- 0
        dummy[i] ~ dpois( phi[i] )
      }

      P[1:2] ~ ddirch(alpha[])     # dirichlet distribution (here, just 2 clusters)
      alpha[1] <- 0.5           
      alpha[2] <- 0.5              

      lambdaMu[1] ~ dunif(0,pi2)
      lambdaMu[2] ~ dunif(pi_1,pi)

      lambdaKappa[1] <- 25         # known concentration for the von Mises
      lambdaKappa[2] <- 100        # the second cluster does not have a kappa
                                   # but a value must be given, so let's choose a high
                                   # concentration to make it harder to be selected

      lambdaI0[1] <- 5774560606.0  # I_0(25)
      lambdaI0[2] <- 1.0E+42       # I_0(100)

      lambdaRho[1] <- 0.999999     # the first cluster does not have a rho parameter
                                   # but a value must be given, so let's choose a high
                                   # concentration to make it harder to be selected
      lambdaRho[2] ~ dunif(0,1)

      # other values

      C    <- 1000000   # for the zero's trick
      pi   <- 3.14159
      pi2  <- 2*pi  
      pi_1 <- -pi
  }
"
```

```{r}
# this rotation is irrelevant for the von Mises inference
pp_my_mixdata <- sapply(as.numeric(my_mixdata), cauchy_process_val)

run.model(modelString, samples=c("lambdaMu","lambdaRho","G"), 
          data=list(x = pp_my_mixdata,
                    n = length(pp_my_mixdata),
                    G = c(1, rep(NA,length(pp_my_mixdata)-1))),
          chainLength=1e4)
```

```{r, collapse=TRUE}
mu_clusters <- samplesStats("lambdaMu")$mean
mu_clusters <- sapply(mu_clusters, cauchy_process_val) # place the mu's between [-pi,pi]

vonMises_mu <- mu_clusters[1]                          # get parameter's point estimates
wrapped_mu  <- mu_clusters[2]
wrapped_rho <- samplesStats("lambdaRho")$mean
```

Let's compare the real signal with the estimated one:

```{r}
ff.real1 <- function(x) dvonmises     (x, mu=circular(mu1_real),  kappa=kappa1_real)
ff.real2 <- function(x) dwrappedcauchy(x, mu=circular(mu2_real),  rho=rho2_real)
ff.real  <- function(x) ifelse(ff.real1(x)>ff.real2(x),ff.real1(x),ff.real2(x))
curve.circular(ff.real, col="blue", shrink=2.5, lwd=2) 

ff.inferred1 <- function(x) dvonmises     (x, mu=circular(vonMises_mu),  kappa=kappa1_real)
ff.inferred2 <- function(x) dwrappedcauchy(x, mu=circular(wrapped_mu), rho=wrapped_rho)
ff.inferred  <- function(x) ifelse(ff.inferred1(x)>ff.inferred2(x),ff.inferred1(x),ff.inferred2(x))
curve.circular(ff.inferred, col="red", lwd=2, add=T) 

legend("topleft",c("real","estimate"), col=c("blue","red"), lwd=2, bty = "n")
```

And let's classify the datapoints:

```{r, collapse=TRUE}
cluster_id <- samplesStats("G")[,1]
cluster_id

estimated_id <- 0+(cluster_id > 1.5) # values are between 1 and 2, we split at the middle

real_id <- c(rep('a',100), rep('b',100))[-1] # remove the first value (was fixed in BUGS)
table(estimated_id, real_id)
```

Again, the color square gives its real cluster id, and the color cross the estimated id:

```{r, warning=FALSE}
mixdata_plot <- my_mixdata[-1]
rose.diag(my_mixdata, bins=50, prop=2.5, col="lightgreen",
          control.circle=circle.control(lty=0))
points(as.circular(mixdata_plot[real_id=='a']), col="orange", pch=0, lwd=1.5)
points(as.circular(mixdata_plot[real_id=='b']), col="purple", pch=0, lwd=1.5)

points(as.circular(mixdata_plot[estimated_id==0]), col="orange", pch=3, lwd=1.5)
points(as.circular(mixdata_plot[estimated_id==1]), col="purple", pch=3, lwd=1.5)
```

Jones and Pewsey distribution
--------------

Ref:

+ Arthur Pewsey et al., [Circular Statistics in R](http://circstatinr.st-andrews.ac.uk/); [google books](https://books.google.pt/books?id=qeadAAAAQBAJ&printsec=frontcover#v=onepage&q&f=false)

This distribution has several known circular distributions as special cases.

Its density is proportional to

$$f(x|\mu,\kappa,\psi) \propto (1+\tanh(\kappa \psi) \cos(x-\mu))^{1/\psi}$$

with $\mu-\pi \lt x \leq \mu+\pi$, $\kappa \geq 0$, \psi \in \mathcal{R}$

However the normalizing constant is more complex (details at their paper, [Jones and Pewsey: Symmetric Distributions on the Circle](http://www.jstor.org/stable/27590682)). The next function computes it:

```{r, collapse=TRUE}
# compute the normalization constact of the Jones and Pewsey distribution
# ref:   Circular Statistics in R, 
# check: https://circstatinr.st-andrews.ac.uk/resources/Chap4-RCommands.txt
JPNCon <- function(kappa, psi){
  ncon<-0
  
  if (kappa < 0.001)
    ncon <- 1/(2*pi) 
  else {
    eps <- 10 * .Machine$double.eps # the smallest positive floating-point x such that 1+x!=1
    if (abs(psi) <= eps)
      ncon <- 1/(2*pi*I.0(kappa))
    else {
      intgrnd <- function(x){ (cosh(kappa*psi)+sinh(kappa*psi)*cos(x))**(1/psi) }
      
      tryCatch(
        {
          ncon <- 1/integrate(intgrnd, lower=-pi, upper=pi)$value
        },
        error = function(c) {
          # some pairs gave integration error at zero (?), so I split the integration in two
          # however this does not solve all the problems...
          sum1 <- integrate(intgrnd, lower=-pi, upper=0)$value
          sum2 <- integrate(intgrnd, lower=0, upper=pi)$value
          ncon <<- 1/(sum1+sum2)  # update variable from parent's environment
        })
    } 
  }
  
  finally = {  # return whether there was an error or not
    return(ncon)
  }
}

JPNCon(kappa=2,psi=0)
JPNCon(kappa=5,psi=-5) 
```

The `circular` package implements this distribution's density. 

Let's see some special cases:

```{r, warning=FALSE}
ff <- function(x) djonespewsey(x, mu=0, kappa=0, psi=0)
curve.circular(ff, col="blue", lwd=2, shrink = 2, 
               main=expression(paste(kappa , " = 0 is the Uniform")))

ff <- function(x) djonespewsey(x, mu=0, kappa=5, psi=0.01)
curve.circular(ff, col="blue", lwd=2, shrink = 2, 
               main=expression(paste(psi," -> 0 approaches the von Mises")))

ff <- function(x) djonespewsey(x, mu=0, kappa=5, psi=1)
curve.circular(ff, col="blue", lwd=2, shrink = 2, 
               main=expression(paste(psi," = 0 is the Cardoid distribution")))

ff <- function(x) djonespewsey(x, mu=0, kappa=2, psi=-1)
curve.circular(ff, col="blue", lwd=2, shrink = 2, 
               main=expression(paste(psi," = -1 is the wrapped Cauchy")))

ff <- function(x) djonespewsey(x, mu=0, kappa=10, psi=10)
curve.circular(ff, col="blue", lwd=2, shrink = 2, 
               main=expression(paste(psi," > 0 & ", kappa , ">>0 is the Cartwright's Power-of-Cosine")))

ff <- function(x) djonespewsey(x, mu=0, kappa=10, psi=-3)
curve.circular(ff, col="blue", lwd=2, shrink = 2,
               main=expression(paste(psi," < -2 & ", kappa , ">>0 is a new distribution")))
```

The package `circular` does not provide a sampling function. The next code uses the method of acceptance-rejection to sample from it:

```{r, warning=FALSE}
# ref:   Circular Statistics in R, 
# check: https://circstatinr.st-andrews.ac.uk/resources/Chap4-RCommands.txt
rjonespewsey <- function(n, mu, kappa, psi) {

  fmax  <- djonespewsey(mu, as.circular(mu), kappa, psi)
  theta <- 0
  
  for (j in 1:n) {
    stopgo <- FALSE
    while (!stopgo) {
      u1 <- runif(1, 0, 2*pi)
      pdfu1 <- djonespewsey(u1, as.circular(mu), kappa, psi)
      u2 <- runif(1, 0, fmax)
      if (u2 <= pdfu1) { 
        theta[j] <- u1 
        stopgo <- TRUE 
      }
    }
  }
  
  as.circular(theta)
}

# Eg of use:
set.seed(121)
mu <- pi/4; kappa <- 2; psi <- -3/4
rdata <- rjonespewsey(100, mu=mu, kappa=kappa, psi=psi)
rose.diag(rdata, bins=50, prop=1.5, shrink=1.5)
points(rdata, pch=20, col="darkgreen")
ff <- function(x) djonespewsey(x, mu=mu, kappa=kappa, psi=psi)
curve.circular(ff, col="blue", lwd=2, add=T)
```

Concerning `circular::djonespewsey`, I did find some integration problems for specific values of ($\mu, \kappa, \psi$), like `djonespewsey(0,0,4.63,-1.61)` produces the error `Error in integrate(ker, 0, 2 * pi) : the integral is probably divergent`.

The package implementation is similar to this one:

```{r, error=TRUE, warning=FALSE}
djonespewsey2 <- function(x, mu, kappa, psi) {
  ker <- function(x) {
    (cosh(kappa * psi) + sinh(kappa * psi) * cos(x - mu))^(1/psi)/
      (2 * pi * cosh(kappa * psi))
  }
  
  normalization <- integrate(ker, 0, 2 * pi)$value
  ker(x)/normalization
}

djonespewsey(0,0,4.63,-1.61)
djonespewsey2(0,0,4.63,-1.61)
```

So, here's a more stable density approximation without using `integrate`:

```{r}
my_djonespewsey <- function(x, mu, kappa, psi) {
  ker <- function(x) {
    (cosh(kappa * psi) + sinh(kappa * psi) * cos(x - mu))^(1/psi)/
      (2 * pi * cosh(kappa * psi))
  }
  
  dx = 0.01
  xs <- seq(0,2*pi,dx)
  normalization <- sum(ker(xs))*dx
  ker(x)/normalization
}

my_djonespewsey(0,0,4.63,-1.61)
```

Just an eg that the stable variant produces similar values to the package implementation:

```{r, warning=FALSE}
mu <- -1; kappa <- 10; psi <- 1

xs <- seq(-pi,pi,len=100)
plot(xs, djonespewsey(xs,mu,kappa,psi), col="blue", lwd=7, type="l")
points(xs, my_djonespewsey(xs,mu,kappa,psi), col="orange", lwd=2, type="l")
```

Stan Model
---------

Due to the difficult normalization constant, I didn't find a practical way to model this distribution on BUGS. The only possibility I see is to provide BUGS with a grid of values $(\kappa, \psi)$ computed by function `JPNCon`, and let BUGS chose the best pair value...

That way, I used Stan to implement the previous stable density, and so we can infer the parameters of this distribution, like we did previously.

```{r, message=FALSE}
library(rstan)
```

This is the model in Stan:

```{r}
model <- '
  functions {

     // debug purposes
     // void print_xs(real[] xs) {
     //   int size_xs;

     //   size_xs <- size(xs);
     //   for(i in 1:size_xs) {
     //      print(xs[i])
     //   }
     // }

     real kernel(real x, real mu, real kappa, real psi) {

        return (cosh(kappa * psi) + sinh(kappa * psi) * cos(x - mu))^(1/psi) /
               (2 * pi() * cosh(kappa * psi));
     }

     real normalization(real dx, real[] xs, real mu, real kappa, real psi) {

        real value;
        int size_xs;

        size_xs <- size(xs);
        value   <- 0;
        for (i in 1:size_xs) {
           value <- value + kernel(xs[i], mu, kappa, psi);
        }
        return value*dx;
     }

     real JonesPewsey_log(real[] D, int N, real dx, real[] xs, 
                          real mu, real kappa, real psi) {

        real log_lik;
        real log_pdf_const;

        log_pdf_const <- log( normalization(dx, xs, mu, kappa, psi) );
        log_lik <- 0;
        for(i in 1:N) {
          //log_lik <- log_lik + log( kernel(D[i], mu, kappa, psi)/pdf_const );
          log_lik <- log_lik + log( kernel(D[i], mu, kappa, psi) ) - log_pdf_const;
        }

        return log_lik;
     }
  }

  data {
    int<lower=0> N;   // number of observed data points
    real D[N];         // the observed data
  }

  transformed data {
 
    real<lower=0> dx; // should be a power of 10: 0.01, 0.001, ...
    real xs[100];     // should have size 1/dx

    dx <- 0.01;       // step to approximate integral with sums

    xs[1] <- 0;       // a grid of points between [0,2pi]
    for(i in 2:100)  {
      xs[i] <- xs[i-1] + 2*pi()*dx;
    }
    // print_xs(xs) // debug: to check if xs was correctly constructed
  }

  parameters {
    real<lower=-pi(), upper=pi()> mu;     // mu over the unit circle
    real<lower= 0,    upper=5>    kappa;  // kappa >= 0
    real<lower=-10,  upper=10>    psi;    // limited to [-10,10]
  }

  model {

    // priors
    mu    ~ uniform(-pi(),pi());   
    kappa ~ uniform( 0,5);
    psi ~ uniform(-10,10);                // behaves better than psi ~ cauchy(0,10);

    // likelihood
    D ~ JonesPewsey(N, dx, xs, mu, kappa, psi);   
  }
'
```

So, let's try to fit some circular data samples. The next function receives parameter values for the Jones and Pewsey distribution, generates a ramdon sample from it, and then fit this sample (assuming the parameter values were unknown), and then plot the true distribution against the estimated one (ie, the distribution which parameter values are the posterior means):

```{r}
# place all angles between 0 <= mu < 2*pi
normalize_angles <- function(x) {
  while(x>2*pi)
    x = x-2*pi
  x
}

fit_and_plot <- function(mu, kappa, psi, N=75, iter=5000) {
  
  # step 1: create a random sample D
  set.seed(121)
  D <- as.numeric(rjonespewsey(N, mu=mu, kappa=kappa, psi=psi))
  D <- sapply(D, normalize_angles) # values between 0 <= mu < 2*pi
  
  # step 2: fit D using Stan's model
  fit  <- stan(model_code = model, data = list(D=D, N=N),  iter = 1000, chains = 1, verbose = FALSE)
  fit2 <- stan(fit = fit,          data = list(D=D, N=N),  iter = iter, chains = 2, 
             verbose = FALSE, seed=101, warmup=1000)
  print(fit2)
  
  # step 2a: extract posterior means
  la <- extract(fit2, permuted = TRUE) 
  mu_hat    <- mean(la$mu)
  kappa_hat <- mean(la$kappa)
  psi_hat   <- mean(la$psi)

  # step 3: plot
  # left panel
  par(mfrow=c(1,2))
  xs <- seq(-pi,pi,len=100)
  plot(xs, my_djonespewsey(xs,mu,kappa,psi), col="blue", lwd=4, type="l",
       xlab="x", ylab="density")
  points(xs, my_djonespewsey(xs,mu_hat,kappa_hat,psi_hat), col="red", lwd=2, type="l")
  # right panel
  plot(as.circular(D), shrink=1.25, col="darkgreen", pch=20)
  ff     <- function(x) my_djonespewsey(x, mu=mu, kappa=kappa, psi=psi)
  curve.circular(ff, col="blue", lwd=4, add=T)
  ff_hat <- function(x) my_djonespewsey(x, mu=mu_hat, kappa=kappa_hat, psi=psi_hat)
  curve.circular(ff_hat, col="red", lwd=2, add=T)
  legend("topleft",c("real","estimate"), col=c("blue","red"), lwd=2, bty = "n")
  par(mfrow=c(1,1))
  
  # return true parameters and point estimates
  data.frame(true=c(mu, kappa, psi), estimate=c(mu_hat, kappa_hat, psi_hat))
}
```

Fitting a distribution close to a von Mises:

```{r, message=FALSE, warning=FALSE}
fit_and_plot(mu=pi/4,  kappa=4, psi=1/8, iter=2500)
fit_and_plot(mu=-pi/2, kappa=4, psi=1/8, iter=2500)
```

Fitting a wrapped Cauchy:

```{r, message=FALSE, warning=FALSE}
fit_and_plot(mu=pi/2, kappa=3, psi=-1, iter=2500)
```

**TODO** The model has trouble fitting for different values of $\mu$. Eg, fitting a Cartwright's Power of Cosine:

```{r, message=FALSE, warning=FALSE}
fit_and_plot(mu=-pi/2  , kappa=5, psi=2)
fit_and_plot(mu=0      , kappa=5, psi=2)
fit_and_plot(mu=-3*pi/4, kappa=5, psi=2)
```

We notice that even when the true distribution is the same -- only an angular shift was performed -- the fitness performance is quite different.

<!-- This was my first try with BUGS. Then I chose Stan

BUGS model (TO DO)
--------

Due to the difficult normalization constant, I didn't find a practical way to model this distribution on BUGS. The only possibility I see is to provide BUGS with a grid of values $(\kappa, \psi)$ computed by function `JPNCon`, and let BUGS chose the best pair value...

```{r, collapse=TRUE, eval=FALSE, echo=FALSE}
# logarithmic sequence
lseq <- function(from,to,len) exp(seq(log(from),log(to),len=len))

# compute grid of normalization constants
grid.size <- 20 # even number
kappas <- lseq(0.001,8,grid.size)
psis   <- lseq(8,0.001,grid.size/2)
psis   <- sort(c(-psis,psis))

grid.NC <- matrix(rep(NA,grid.size^2), nrow=grid.size)
for(row in 1:grid.size)
  for(col in 1:grid.size)
    grid.NC[row,col] <- JPNCon(kappas[row], psis[col])
    
round(grid.NC[10:20,1:8],5)
```

Any help would be appreciated.

```{r, eval=FALSE, echo=FALSE}
my_data <- rvonmises(100, circular(3*pi/2), 2.5)
rose.diag(my_data, bins=50, prop=1.5, shrink=1)
points(my_data, pch=20, col="red")
```


```{r, eval=FALSE, echo=FALSE}
modelString = "
  model {

      for (i in 1:n) {

        # this formula includes only the density kernel, it lacks the normalization constant
#         phi[i] <- -log( pow(1+((1-exp(-2*kappa*psi))/(1+exp(-2*kappa*psi)))*cos(x[i]-mu)  
#                            ,(1/psi) ) ) + C
        
        phi[i] <- -log( 1 + ((1-exp(-2*kappa*psi))/(1+exp(-2*kappa*psi))) * cos(x[i]-mu) ) 
                  * (1/psi) + C


        dummy[i] <- 0
        dummy[i] ~ dpois( phi[i] )
      }

      # Priors 

      mu    ~ dunif(pi_1,pi)
      kappa ~ dunif(0,10)
      psi   ~ dunif(-10,10)

      # other values

      C   <- 1000000   # for the zero's trick
      pi  <- 3.14159
      pi_1 <- -pi
  }
"
```

```{r, eval=FALSE, echo=FALSE}
pp_my_data <- sapply(as.numeric(my_data), cauchy_process_val)

# initializations
genInitFactory <- function()  {
  i <- 0
  function() {
    i <<- i + 1
    list( 
      mu = mean(pp_my_data), # start with the mean estimate
      kappa=5,
      psi=0
    ) 
  }
}

# Everything is ready. Run the model!
run.model(modelString, samples=c("mu"), 
          data=list(x     = pp_my_data,
                    n     = length(pp_my_data)), 
          chainLength=5e3, init.func=genInitFactory())

```

-->